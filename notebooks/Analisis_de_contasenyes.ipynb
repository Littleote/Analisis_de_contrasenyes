{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5695a185",
   "metadata": {},
   "source": [
    "# CAS KAGGLE: Analisis de la seguretat de contrasenyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bbb0d",
   "metadata": {},
   "source": [
    "### David Candela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b3810",
   "metadata": {},
   "source": [
    "# Introducció\n",
    "\n",
    "Que faig i perquè."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2258b5",
   "metadata": {},
   "source": [
    "# 1. Netejar i visualitzar el dataset\n",
    "\n",
    "https://www.kaggle.com/bhavikbb/password-strength-classifier-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuscules = [chr(c) for c in range(ord('a'), ord('z') + 1)]\n",
    "Majuscules = [chr(c) for c in range(ord('A'), ord('Z') + 1)]\n",
    "Xifres = [str(i) for i in range(10)]\n",
    "Especials = ['.', ';', '-', '_', '+', '*', '<', '>', '[', ']', '{', '}', \\\n",
    "             '(', ')', '@', '#', '$', '%', '&', '/', '\\\\', '?', '!', '=', \\\n",
    "             '^', '~', ' ']\n",
    "CaractersValids = Minuscules + Majuscules + Xifres + Especials\n",
    "TipusCaracters = dict()\n",
    "TipusCaracters.update({c: \"Minuscules\" for c in Minuscules})\n",
    "TipusCaracters.update({c: \"Majuscules\" for c in Majuscules})\n",
    "TipusCaracters.update({c: \"Xifres\" for c in Xifres})\n",
    "TipusCaracters.update({c: \"Especials\" for c in Especials})\n",
    "\n",
    "def isValid(contrasenya):\n",
    "    try:\n",
    "        # Treure les contrasenyes amb caracters que no consideri valids\n",
    "        #   ja que no importa el format de descodificació especificat,\n",
    "        #   Python es incapaç de llegir correctament tots els diferents caràcters\n",
    "        #   caracters i sempre en surten de l'estil '\\x03', '\\x0f', '\\x8d'\n",
    "        #   o també §, ¶, ­, þ, ¤, ...\n",
    "        return all(c in CaractersValids for c in contrasenya)\n",
    "    except:\n",
    "        # Truere les contrasenyes que Pandas converteixi continuament a float\n",
    "        #   tot i que s'ha marcat la columna de passwords com strings\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e882b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = '../data/'\n",
    "data_name = 'data.csv'\n",
    "clean_name = 'clean_data.csv'\n",
    "data_file = path + data_name\n",
    "clean_file = path + clean_name\n",
    "regenerate_file = False\n",
    "\n",
    "if regenerate_file or not os.path.isfile(clean_file):\n",
    "    assert os.path.isfile(data_file)\n",
    "    # Saltarse les files on les dades no estiguin ben formategades\n",
    "    dataset = pd.read_csv(data_file, on_bad_lines='skip', encoding='utf-8', dtype={'password': str, 'strength': np.int64})\n",
    "    # Treure les dades que contenen caracters que no acceptem\n",
    "    dataset = dataset[dataset.apply(lambda s: isValid(s['password']), axis=1)]\n",
    "    # Guardar el nou dataset a un fitxer apart\n",
    "    dataset.to_csv(clean_file, index=False)\n",
    "    \n",
    "dataset = pd.read_csv(clean_file)\n",
    "dades = dataset.values\n",
    "contrasenyes = dades[:,0]\n",
    "dades_proteccio = dades[:,1].astype(np.int64)\n",
    "noms = dataset.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af464d3",
   "metadata": {},
   "source": [
    "# 2. Extreure dades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87fb03",
   "metadata": {},
   "source": [
    "## Predictabilitat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a33cf",
   "metadata": {},
   "source": [
    "Primer mirarem que tan probables i que tan facils de predir les contrasenyes.  \n",
    "En concret mirarem:\n",
    "* Probabilitat dels caracters que utilitza\n",
    "* Probabilitat d'una sequencia com aquesta\n",
    "* Com de barrejats estàn els carcters\n",
    "* Que tan llarga és la contrasenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompte(contrasenya, individual, predictabilitat):\n",
    "    c_ = ''\n",
    "    for c in contrasenya:\n",
    "        predictabilitat[c_][c] += 1\n",
    "        predictabilitat[c_]['total'] += 1\n",
    "        individual[c] += 1\n",
    "        individual['total'] += 1\n",
    "        c_ = c\n",
    "\n",
    "individual = {c:0 for c in CaractersValids + ['total']}\n",
    "predictabilitat = {c:{c:1 if not c == 'total' else len(CaractersValids) for c in CaractersValids + ['total']} for c in CaractersValids + ['']}\n",
    "\n",
    "for p in dades[:,0]:\n",
    "    recompte(p, individual, predictabilitat)\n",
    "\n",
    "individual_total = individual['total']\n",
    "predictabilitat_total = {c:predictabilitat[c]['total'] for c in predictabilitat.keys()}\n",
    "\n",
    "individual.pop('total')\n",
    "for k in predictabilitat.keys():\n",
    "    predictabilitat[k].pop('total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "plt.subplots(figsize=(20, 18))\n",
    "plt.bar(individual.keys(), individual.values())\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap([[e for e in d.values()] for d in predictabilitat.values()], \\\n",
    "            xticklabels=CaractersValids, yticklabels=CaractersValids + ['inici'], square=True, norm=LogNorm())\n",
    "plt.xlabel(\"Segon\")\n",
    "plt.ylabel(\"Primer\")\n",
    "plt.title(\"Sequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089e560",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def probabilitat_caracters(x):\n",
    "    res = 1\n",
    "    valid = 0\n",
    "    for c in x:\n",
    "        res *= individual.get(c, 1)\n",
    "        valid += 1\n",
    "    return res / (individual_total ** valid)\n",
    "\n",
    "def probabilitat_sequencia(x):\n",
    "    res = 1\n",
    "    c_ = ''\n",
    "    for c in x:\n",
    "        if c not in CaractersValids:\n",
    "            continue\n",
    "        res *= predictabilitat[c_][c] / predictabilitat_total[c_]\n",
    "        c_ = c\n",
    "    return res\n",
    "\n",
    "def aleatorietat(x):\n",
    "    res = 1\n",
    "    c_ = ''\n",
    "    for c in x:\n",
    "        if c not in CaractersValids:\n",
    "            continue\n",
    "        res *= individual[c] / predictabilitat[c_][c] * predictabilitat_total[c_] / individual_total\n",
    "        c_ = c\n",
    "    return res\n",
    "\n",
    "def llargada(x):\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "dades_caracters = np.vectorize(probabilitat_caracters)(contrasenyes)\n",
    "dades_sequencia = np.vectorize(probabilitat_sequencia)(contrasenyes)\n",
    "dades_aleatorietat = np.vectorize(aleatorietat)(contrasenyes)\n",
    "dades_llargada = np.vectorize(llargada)(contrasenyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DP = np.stack((dades_caracters, dades_sequencia, dades_aleatorietat, dades_llargada, dades_proteccio), axis=-1)\n",
    "dfDP = pd.DataFrame(data=DP, columns=['caracters', 'sequencia', 'aleatorietat', 'llargada', 'proteccio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"caracters\", hue=\"proteccio\", multiple=\"layer\", log_scale=True, binrange=[-39, -7], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292135b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"sequencia\", hue=\"proteccio\", multiple=\"layer\", log_scale=True, binrange=[-36, -5], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73070de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"aleatorietat\", hue=\"proteccio\", multiple=\"layer\", log_scale=True, binrange=[-10, 6], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"llargada\", hue=\"proteccio\", multiple=\"stack\", discrete=True, binrange=[1, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8042c9e",
   "metadata": {},
   "source": [
    "### Probar eficacia de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(X, y, p=1, index=None, returnUnusedIndex=False):\n",
    "    if index is not None:\n",
    "        l = len(index)\n",
    "        idx = returnUnusedIndex\n",
    "    else:\n",
    "        l = len(y)\n",
    "        index = np.array([i for i in range(l)])\n",
    "        idx = False\n",
    "    if p == 1:\n",
    "        new_X = X[index]\n",
    "        new_y = y[index]\n",
    "        index = np.array([])\n",
    "        unique, counts = np.unique(new_y, return_counts=True)\n",
    "        values = {u: l / len(unique) / c for u, c in zip(unique, counts)}\n",
    "        ret = (new_X, new_y, np.array([values[yi] for yi in new_y]))\n",
    "    elif type(p) == list:\n",
    "        subsets = []\n",
    "        f = 1\n",
    "        for pi in p:\n",
    "            elem, index = make_subset(X, y, p=pi / f, index=index, returnUnusedIndex=True)\n",
    "            subsets.append(elem)\n",
    "            f -= pi\n",
    "        subsets.append(make_subset(X, y, index=index))\n",
    "        ret = (subsets)\n",
    "    else:\n",
    "        new_l = int(l * p)\n",
    "        ind = np.random.choice(index, new_l, replace=False)\n",
    "        index = np.setdiff1d(index, ind)\n",
    "        new_X = X[ind]\n",
    "        new_y = y[ind]\n",
    "        unique, counts = np.unique(new_y, return_counts=True)\n",
    "        values = {u: new_l / len(unique) / c for u, c in zip(unique, counts)}\n",
    "        ret = (new_X, new_y, np.array([values[yi] for yi in new_y]))\n",
    "    if idx:\n",
    "        return ret, index\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "def Wt(w, n):\n",
    "    return w * n / len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac416db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class LogPipe:\n",
    "    err = np.float64(1e-200)\n",
    "    def __init__ (self, columns):\n",
    "        self.column_transform = columns\n",
    "    \n",
    "    def fit (self, X, **args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, copy=True):\n",
    "        if copy == True:\n",
    "            X_tr = np.copy(X)\n",
    "        else:\n",
    "            X_tr = X\n",
    "        X_tr[:, self.column_transform] = np.log(X_tr[:, self.column_transform].astype(np.float64) + LogPipe.err)\n",
    "        return X_tr\n",
    "    \n",
    "    def fit_transform(self, X, y=None, copy=True, **args):\n",
    "        return self.transform(X, copy)\n",
    "\n",
    "def visualize_confusion_matrix(y_pred, y_real, w=None):\n",
    "    #mostra la matriu de confusió\n",
    "    cm = confusion_matrix(y_real, y_pred, sample_weight=w)\n",
    "    plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(cm, annot = True, fmt = 'g')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6, C=100))])\n",
    "\n",
    "atrib1 = ['caracters', 'sequencia', 'aleatorietat', 'llargada']\n",
    "objP = 'proteccio'\n",
    "atribDP = dfDP[atrib1].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "x_t, y_t, w_t = make_subset(atribDP, objDP, .2)\n",
    "x_cv, y_cv, w_cv = make_subset(atribDP, objDP, 1)\n",
    "pipe1.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "prediccio = pipe1.predict(x_cv)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(w_cv * (y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, y_cv, Wt(w_cv, 3))\n",
    "\n",
    "dfP1 = pd.DataFrame(pipe1['model'].coef_, columns=atrib1)\n",
    "dfP1['1'] = pipe1['model'].intercept_\n",
    "dfP1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb59bfe",
   "metadata": {},
   "source": [
    "Veiem que arriba al 100% donant importancia a la llargada, una posible explicació sería que només amb la llargada ja fos suficient per extreure el nivell de protecció tal com ho semblaba al histograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb799824",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f'Per nivell de protecció {i} la llargada es troba entre {min(dades_llargada[dades_proteccio == i])} i {max(dades_llargada[dades_proteccio == i])}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "prediccio = np.array([0 if len(x) < 8 else (2 if len(x) > 13 else 1) for x in contrasenyes])\n",
    "visualize_confusion_matrix(prediccio, dades_proteccio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5bcb7",
   "metadata": {},
   "source": [
    "Fent aquest comporbació veïem que en efecte és posible deduir el nivell de protecció donada la longitud de la contrasenya.\n",
    "Tot i que ara tinguem un mètode que ens dona un 100% de precisió, com, segons diu el dataset, està format pels casos on coincideixen tres algoritmes diferents. LLavors buscarem a veure si hi ha algun altre manera de predir el nivell de seguretat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de32129",
   "metadata": {},
   "source": [
    "### Probar eficacia de les dades (sense longitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727ba23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe1b = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6, C=100))])\n",
    "\n",
    "atrib1b = ['caracters', 'sequencia', 'aleatorietat']\n",
    "objP = 'proteccio'\n",
    "atribDP = dfDP[atrib1b].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "x_t, y_t, w_t = make_subset(atribDP, objDP, .2)\n",
    "x_cv, y_cv, w_cv = make_subset(atribDP, objDP, 1)\n",
    "pipe1b.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "prediccio = pipe1b.predict(x_cv)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(w_cv * (y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, y_cv, Wt(w_cv, 3))\n",
    "\n",
    "dfP1b = pd.DataFrame(pipe1b['model'].coef_, columns=atrib1b)\n",
    "dfP1b['1'] = pipe1b['model'].intercept_\n",
    "dfP1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "kFolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf00f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib1b].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index])\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t) # model__sample_weight=w_t\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28921c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib1b].values\n",
    "objL = 'llargada'\n",
    "objDP = dfDP[objL].values\n",
    "\n",
    "Y_cv, prediccio = [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t = atribDP[train_index], objDP[train_index]\n",
    "    x_cv, y_cv = atribDP[test_index], objDP[test_index]\n",
    "    pipe.fit(x_t, y_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'MSE: {sum((Y_cv - prediccio) ** 2) / len(prediccio)}')\n",
    "print(f'R^2: {r2_score(Y_cv, prediccio)}')\n",
    "fig = plt.figure()\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "ax = plt.gca()\n",
    "plt.scatter(x=prediccio, y=Y_cv, s=1, alpha=.1)\n",
    "plt.plot([0, 13.5], [7.5, 7.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 7.5], [0, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 50], [13.5, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([13.5, 13.5], [7.5, 50], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a19fd4",
   "metadata": {},
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1bdbc",
   "metadata": {},
   "source": [
    "## Predictabilitat (mitja)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48488a8",
   "metadata": {},
   "source": [
    "Per treure la correlació que hi ha entre les dades de probabilitat i de llargada (més caracters més improbable) només calcularemm la mitja d'aquests valors.  \n",
    "Per tant tindrem:\n",
    "* Mitja de la probabilitat dels caracters que utilitza\n",
    "* Mitja de la probabilitat d'una sequencia com aquesta\n",
    "* Com de barrejats estàn els carcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989da13e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mitja_caracters(x):\n",
    "    res = 0\n",
    "    valid = 0\n",
    "    for c in x:\n",
    "        res += np.log(individual.get(c, 1))\n",
    "        valid += 1\n",
    "    return np.exp(res / valid) / individual_total if valid > 0 else 1\n",
    "\n",
    "def mitja_sequencia(x):\n",
    "    res = 1\n",
    "    c_ = ''\n",
    "    valid = 0\n",
    "    for c in x:\n",
    "        if c not in CaractersValids:\n",
    "            continue\n",
    "        res *= predictabilitat[c_][c] / predictabilitat_total[c_]\n",
    "        c_ = c\n",
    "        valid += 1\n",
    "    return res ** (1 / valid) if valid > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dades_mitja_caracters = np.vectorize(mitja_caracters)(contrasenyes)\n",
    "dades_mitja_sequencia = np.vectorize(mitja_sequencia)(contrasenyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDP[['mitja caracters', 'mitja sequencia']] = np.stack((dades_mitja_caracters, dades_mitja_sequencia), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb65e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"mitja caracters\", hue=\"proteccio\", multiple=\"layer\", log_scale=True, binrange=[-2.3, -1.3], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"mitja sequencia\", hue=\"proteccio\", multiple=\"layer\", log_scale=True, binrange=[-2.2, -0.8], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2afa52",
   "metadata": {},
   "source": [
    "### Probar eficacia de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6, C=100))])\n",
    "\n",
    "atrib2 = ['mitja caracters', 'mitja sequencia', 'aleatorietat']\n",
    "atribDP = dfDP[atrib2].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "x_t, y_t, w_t = make_subset(atribDP, objDP, .2)\n",
    "x_cv, y_cv, w_cv = make_subset(atribDP, objDP, 1)\n",
    "pipe2.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "prediccio = pipe2.predict(x_cv)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(w_cv * (y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, y_cv, Wt(w_cv, 3))\n",
    "\n",
    "dfP2 = pd.DataFrame(pipe2['model'].coef_, columns=atrib2)\n",
    "dfP2['1'] = pipe2['model'].intercept_\n",
    "dfP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c408e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib2].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index])\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t) # model__sample_weight=w_t\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8005edb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib2].values\n",
    "objDP = dfDP[objL].values\n",
    "\n",
    "Y_cv, prediccio = [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t = atribDP[train_index], objDP[train_index]\n",
    "    x_cv, y_cv = atribDP[test_index], objDP[test_index]\n",
    "    pipe.fit(x_t, y_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'MSE: {sum((Y_cv - prediccio) ** 2) / len(prediccio)}')\n",
    "print(f'R^2: {r2_score(Y_cv, prediccio)}')\n",
    "fig = plt.figure()\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "ax = plt.gca()\n",
    "plt.scatter(x=prediccio, y=Y_cv, s=1, alpha=.1)\n",
    "plt.plot([0, 13.5], [7.5, 7.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 7.5], [0, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 50], [13.5, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([13.5, 13.5], [7.5, 50], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497efce1",
   "metadata": {},
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29e479",
   "metadata": {},
   "source": [
    "## Cadenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd4d11",
   "metadata": {},
   "source": [
    "Per mirar que tant variada és una contrasenya també es pot fer mirant quantes vegdes cambia o durant quant és manté utilitzant un tipus de caracters en concret (majuscules, minuscules, xifres i caracters especials).  \n",
    "En aquest cas mirarem quant duren de mitja aquestes cadenes del mateix tipus de caracters de dues formes:\n",
    "* Mitja de la longitud de les cadenes\n",
    "* Mitja ponderada de la longitud de les cadenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47789b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mitja_cadena(x):\n",
    "    canvis = 0\n",
    "    valid = 0\n",
    "    ultim = \"Cap\"\n",
    "    for c in x:\n",
    "        actual = TipusCaracters.get(c, \"Cap\")\n",
    "        if actual == \"Cap\":\n",
    "            continue\n",
    "        elif not actual == ultim:\n",
    "            ultim = actual\n",
    "            canvis += 1\n",
    "        valid += 1\n",
    "    return valid / canvis if not canvis == 0 else 0\n",
    "\n",
    "def ponderacio_cadena(x):\n",
    "    valid = 0\n",
    "    S, s = 0, 1\n",
    "    ultim = \"Cap\"\n",
    "    for c in x:\n",
    "        actual = TipusCaracters.get(c, \"Cap\")\n",
    "        if actual == \"Cap\":\n",
    "            continue\n",
    "        elif not actual == ultim:\n",
    "            ultim = actual\n",
    "            s = 1\n",
    "        else:\n",
    "            s += 2\n",
    "        S += s\n",
    "        valid += 1\n",
    "    return S / valid if not valid == 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dades_mitja_cadena = np.vectorize(mitja_cadena)(contrasenyes)\n",
    "dades_ponderacio_cadena = np.vectorize(ponderacio_cadena)(contrasenyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDP[['mitja cadena', 'ponderacio cadena']] = np.stack((dades_mitja_cadena, dades_ponderacio_cadena), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f93eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"mitja cadena\", hue=\"proteccio\", multiple=\"stack\", binrange=[1, 10])\n",
    "plt.show()\n",
    "sns.histplot(data=dfDP, x=\"mitja cadena\", hue=\"proteccio\", multiple=\"layer\", binwidth=1, binrange=[1, 10], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64406f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"ponderacio cadena\", hue=\"proteccio\", multiple=\"stack\", binrange=[1, 10])\n",
    "plt.show()\n",
    "sns.histplot(data=dfDP, x=\"ponderacio cadena\", hue=\"proteccio\", multiple=\"layer\", binwidth=1, binrange=[0, 10], element=\"poly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecba513",
   "metadata": {},
   "source": [
    "### Probar eficacia de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6, C=100))])\n",
    "\n",
    "atrib3 = ['mitja cadena', 'ponderacio cadena']\n",
    "atribDP = dfDP[atrib3].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "x_t, y_t, w_t = make_subset(atribDP, objDP, .2)\n",
    "x_cv, y_cv, w_cv = make_subset(atribDP, objDP, 1)\n",
    "pipe3.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "prediccio = pipe3.predict(x_cv)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(w_cv * (y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, y_cv, Wt(w_cv, 3))\n",
    "\n",
    "dfP3 = pd.DataFrame(pipe3['model'].coef_, columns=atrib3)\n",
    "dfP3['1'] = pipe3['model'].intercept_\n",
    "dfP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4ffba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib3].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index])\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb4b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib3].values\n",
    "objDP = dfDP[objL].values\n",
    "\n",
    "Y_cv, prediccio = [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t = atribDP[train_index], objDP[train_index]\n",
    "    x_cv, y_cv = atribDP[test_index], objDP[test_index]\n",
    "    pipe.fit(x_t, y_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'MSE: {sum((Y_cv - prediccio) ** 2) / len(prediccio)}')\n",
    "print(f'R^2: {r2_score(Y_cv, prediccio)}')\n",
    "fig = plt.figure()\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "ax = plt.gca()\n",
    "plt.scatter(x=prediccio, y=Y_cv, s=1, alpha=.1)\n",
    "plt.plot([0, 13.5], [7.5, 7.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 7.5], [0, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 50], [13.5, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([13.5, 13.5], [7.5, 50], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df5402",
   "metadata": {},
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8feec",
   "metadata": {},
   "source": [
    "## Tipus de caracters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620f302",
   "metadata": {},
   "source": [
    "Finalment mirarem quins tipus de caracters utilitza (majuscules, minuscules, xifres i caracters especials) ja que aquesta és una de les coses que en diversos llocs s'asseguren de que facis per que tinguis una contrasenya segura.  \n",
    "Per tant els últims seràn:\n",
    "* Conté alguna minuscula\n",
    "* Conté alguna majuscula\n",
    "* Conté alguna xifra\n",
    "* Conté algun caracter especial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teMinuscules(contrasenya):\n",
    "    return any(c in Minuscules for c in contrasenya)\n",
    "\n",
    "def teMajuscules(contrasenya):\n",
    "    return any(c in Majuscules for c in contrasenya)\n",
    "\n",
    "def teXifres(contrasenya):\n",
    "    return any(c in Xifres for c in contrasenya)\n",
    "\n",
    "def teEspecials(contrasenya):\n",
    "    return any(c in Especials for c in contrasenya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d724559",
   "metadata": {},
   "outputs": [],
   "source": [
    "dades_minuscules = np.vectorize(teMinuscules)(contrasenyes)\n",
    "dades_majuscules = np.vectorize(teMajuscules)(contrasenyes)\n",
    "dades_xifres = np.vectorize(teXifres)(contrasenyes)\n",
    "dades_especials = np.vectorize(teEspecials)(contrasenyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDP[['minuscules', 'majuscules', 'xifres', 'especials']] = np.stack((dades_minuscules, dades_majuscules, dades_xifres, dades_especials), axis=-1)\n",
    "dfDP['caracter_flags'] = dfDP['minuscules'] + 2 * dfDP['majuscules'] + 4 * dfDP['xifres'] + 8 * dfDP['especials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"minuscules\", hue=\"proteccio\", multiple=\"dodge\", shrink=.5, discrete=True)\n",
    "ax=plt.gca()\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n",
    "sns.histplot(data=dfDP, x=\"majuscules\", hue=\"proteccio\", multiple=\"dodge\", shrink=.5, discrete=True)\n",
    "ax=plt.gca()\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n",
    "sns.histplot(data=dfDP, x=\"xifres\", hue=\"proteccio\", multiple=\"dodge\", shrink=.5, discrete=True)\n",
    "ax=plt.gca()\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n",
    "sns.histplot(data=dfDP, x=\"especials\", hue=\"proteccio\", multiple=\"dodge\", shrink=.5, discrete=True)\n",
    "ax=plt.gca()\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da86c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dfDP, x=\"caracter_flags\", hue=\"proteccio\", multiple=\"dodge\", shrink=.5, discrete=True)\n",
    "ax=plt.gca()\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1237e",
   "metadata": {},
   "source": [
    "### Probar eficacia de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e06b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4 = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6, C=100))])\n",
    "\n",
    "atrib4 = ['minuscules', 'majuscules', 'xifres', 'especials']\n",
    "atribDP = dfDP[atrib4].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "x_t, y_t, w_t = make_subset(atribDP, objDP, .2)\n",
    "x_cv, y_cv, w_cv = make_subset(atribDP, objDP, 1)\n",
    "pipe4.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "prediccio = pipe4.predict(x_cv)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(w_cv * (y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "visualize_confusion_matrix(prediccio, y_cv, Wt(w_cv, 3))\n",
    "\n",
    "dfP4 = pd.DataFrame(pipe4['model'].coef_, columns=atrib4)\n",
    "dfP4['1'] = pipe4['model'].intercept_\n",
    "dfP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib4].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index])\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t) # model__sample_weight=w_t\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {sum(Y_cv==prediccio) / len(prediccio)}')\n",
    "print(f'Macro accuracy: {sum(W_cv * (Y_cv==prediccio)) / len(prediccio)}')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae62e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "\n",
    "atribDP = dfDP[atrib4].values\n",
    "objDP = dfDP[objL].values\n",
    "\n",
    "Y_cv, prediccio = [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t = atribDP[train_index], objDP[train_index]\n",
    "    x_cv, y_cv = atribDP[test_index], objDP[test_index]\n",
    "    pipe.fit(x_t, y_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'MSE: {sum((Y_cv - prediccio) ** 2) / len(prediccio)}')\n",
    "print(f'R^2: {r2_score(Y_cv, prediccio)}')\n",
    "fig = plt.figure()\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "ax = plt.gca()\n",
    "plt.scatter(x=prediccio, y=Y_cv, s=1, alpha=.1)\n",
    "plt.plot([0, 13.5], [7.5, 7.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 7.5], [0, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 50], [13.5, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([13.5, 13.5], [7.5, 50], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196ebd7",
   "metadata": {},
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66ee58",
   "metadata": {},
   "source": [
    "# 3. Escollir un model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "kFolds = 3\n",
    "set_reduction = .1\n",
    "\n",
    "atrib23 = atrib2 + atrib3\n",
    "atrib34 = atrib3 + atrib4\n",
    "atrib24 = atrib2 + atrib4\n",
    "atribAll = atrib2 + atrib3 + atrib4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423749ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6))])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288bcaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()),\\\n",
    "                 ('PCA', PCA(n_components=5)), ('model', LogisticRegression(max_iter=1e6))])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff33482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6))])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f713be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LinearSVC(max_iter=1e6))])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbf1b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', SVC())])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', GaussianNB())])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fb4e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', BaggingClassifier(LogisticRegression(), max_samples=0.5, max_features=0.5))])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20b01f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', RandomForestClassifier(n_estimators=10, max_depth=20))])\n",
    "\n",
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d285b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', RandomForestClassifier(n_estimators=10, max_depth=20))])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3aab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "def train(model, train_data, optimizer, criterion):\n",
    "    X, y, w = train_data\n",
    "    losses = []\n",
    "    model.train()\n",
    "    kf = KFold(n_splits=5)\n",
    "    for _, index in kf.split(y):\n",
    "        x_b, y_b = torch.Tensor(X[index].astype(np.float64)), torch.LongTensor(y[index].astype(np.int64))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_b)\n",
    "        loss = criterion(output, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "def test(model, test_data, criterion, showConfusionMatrix=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    totals = 0\n",
    "    X, y, w = test_data\n",
    "    Y_cv, W_cv, prediccio = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        kf = KFold(n_splits=5)\n",
    "        for _, index in kf.split(y):\n",
    "            x_b, y_b, w_b = torch.Tensor(X[index].astype(np.float64)), torch.LongTensor(y[index].astype(np.int64)), torch.Tensor(w[index].astype(np.float64))\n",
    "            output = model(x_b)\n",
    "            test_loss += criterion(output, y_b).item() * x_b.shape[0]\n",
    "            pred = output.argmax(dim=1)\n",
    "            Y_cv.append(np.asarray(y_b))\n",
    "            W_cv.append(np.asarray(w_b))\n",
    "            prediccio.append(np.asarray(pred))\n",
    "\n",
    "    Y_cv = np.concatenate(Y_cv)\n",
    "    W_cv = np.concatenate(W_cv)\n",
    "    prediccio = np.concatenate(prediccio)\n",
    "    \n",
    "    test_loss /= len(y)\n",
    "    accuracy = 100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio)\n",
    "    \n",
    "    if showConfusionMatrix:\n",
    "        visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, sum(Y_cv==prediccio), len(prediccio), accuracy))\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def NNTrain(model, epochs, loss, optimizer, train_data, test_data, name=''):\n",
    "    init_time = time.time()\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_test = []\n",
    "    print('--'*50)\n",
    "    print('STARTING TRAINING OF {}'.format(name))\n",
    "    print('--'*50)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    print(\"CHECKING INITIAL TEST LOSS (with random weights..)\")\n",
    "    # calculo aquest test tan sols per visualitzar més maca la gràfica de losses. No caldria utilitzar-lo\n",
    "    loss_test_epoch, accuracy_epoch = test(model, test_data, loss)\n",
    "    losses_test.append(loss_test_epoch)\n",
    "    accuracies_test.append(accuracy_epoch)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print (\"EPOCH {}\".format(epoch))\n",
    "        sys.stdout.flush()\n",
    "        loss_train_epoch = train(model, train_data, optimizer, loss)\n",
    "        if epoch == epochs:\n",
    "            loss_test_epoch, accuracy_epoch = test(model, test_data, loss, showConfusionMatrix=True)\n",
    "        else:\n",
    "            loss_test_epoch, accuracy_epoch = test(model, test_data, loss)\n",
    "\n",
    "        losses_train.extend(loss_train_epoch)\n",
    "        losses_test.append(loss_test_epoch)\n",
    "        accuracies_test.append(accuracy_epoch)\n",
    "\n",
    "    plt.plot(range(len(losses_train)), \n",
    "             losses_train, label=\"Training Loss\")\n",
    "\n",
    "    plt.plot(range(0, len(losses_train)+1, int(len(losses_train)/(len(losses_test)-1))), \n",
    "             losses_test, label=\"Test Loss\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(0, len(accuracies_test), int(len(accuracies_test)/(len(accuracies_test)-1))), \n",
    "             accuracies_test, label=\"Accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    elapsed = time.time()-init_time\n",
    "\n",
    "    print (\"ELAPSED TIME: {:.1f}s\".format(elapsed))\n",
    "\n",
    "    return losses_train, losses_test, accuracies_test, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(NNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(p, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6055b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atribDP = dfDP[atribAll].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "train_data, test_data = make_subset(atribDP, objDP, [1-1/kFolds])\n",
    "\n",
    "learning_rate = 0.01\n",
    "modelNNet = NNet(len(atribAll))\n",
    "loss = nn.CrossEntropyLoss(weight=torch.Tensor([1/sum(train_data[1]==0), 1/sum(train_data[1]==1), 1/sum(train_data[1]==2)]))\n",
    "optimizer = torch.optim.Adam(modelNNet.parameters(), lr=learning_rate)\n",
    "loss_train, loss_test, acc_test, elapsedNL = NNTrain(modelNNet, 50, loss, optimizer, \n",
    "                                                      train_data, test_data, name='NNET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier:\n",
    "    def __init__(self, classifier1, classifier2=None, **classifier_args):\n",
    "        # Separar arguments que només siguin per un\n",
    "        clf1_args = {key if not key.startswith('clf1_') else key[5:] : arg\\\n",
    "                     for key, arg in classifier_args.items() if not key.startswith('clf2_')}\n",
    "        clf2_args = {key if not key.startswith('clf2_') else key[5:] : arg\\\n",
    "                     for key, arg in classifier_args.items() if not key.startswith('clf1_')}\n",
    "        # Utilitzar el mateix classificador si només s'ha indicat un\n",
    "        if classifier2 is None:\n",
    "            classifier2 = classifier1\n",
    "        self.clf1 = classifier1(**clf1_args)\n",
    "        self.clf2 = classifier2(**clf2_args)\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        ind = (y<2)\n",
    "        # Diferenciar entre (0, 1) i (2)\n",
    "        X1, y1, sample_weight1 = X, np.copy(y), sample_weight\n",
    "        y1[ind] = 0\n",
    "        self.clf1.fit(X1, y1, sample_weight=sample_weight1)\n",
    "        # Diferenciar entre (0) i (1)\n",
    "        X2, y2, sample_weight2 = np.copy(X)[ind], np.copy(y)[ind], np.copy(sample_weight)[ind]\n",
    "        self.clf2.fit(X2, y2, sample_weight=sample_weight2)\n",
    "    \n",
    "    def predict(self, X, **predict_params):\n",
    "        pred = self.clf1.predict(X, **predict_params)\n",
    "        ind = (pred<2)\n",
    "        pred[ind] = self.clf2.predict(X[ind], **predict_params)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b083371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', CustomClassifier(LogisticRegression, RandomForestClassifier, clf1_C=1, clf2_n_estimators=10, clf2_max_depth=20))])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "Y_cv, W_cv, prediccio = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "init_time = time.time()\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t, w_t = make_subset(atribDP[train_index], objDP[train_index], set_reduction)\n",
    "    x_cv, y_cv, w_cv = make_subset(atribDP[test_index], objDP[test_index])\n",
    "    pipe.fit(x_t, y_t, model__sample_weight=w_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    W_cv.append(w_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "elapsed = time.time() - init_time\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "W_cv = np.concatenate(W_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'Accuracy: {np.round(100 * sum(Y_cv==prediccio) / len(prediccio), 3)}%')\n",
    "print(f'Macro accuracy: {np.round(100 * sum(W_cv * (Y_cv==prediccio)) / len(prediccio), 3)}%')\n",
    "print(f'Time elapsed: {elapsed}s')\n",
    "visualize_confusion_matrix(prediccio, Y_cv, Wt(W_cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84441fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33a63b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', RandomForestRegressor(n_estimators=10, max_depth=20))])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objL].values\n",
    "\n",
    "Y_cv, prediccio = [], []\n",
    "\n",
    "kf = KFold(n_splits=kFolds)\n",
    "for train_index, test_index in kf.split(objDP):\n",
    "    x_t, y_t = atribDP[train_index], objDP[train_index]\n",
    "    x_cv, y_cv = atribDP[test_index], objDP[test_index]\n",
    "    pipe.fit(x_t, y_t)\n",
    "\n",
    "    Y_cv.append(y_cv)\n",
    "    prediccio.append(pipe.predict(x_cv))\n",
    "    \n",
    "Y_cv = np.concatenate(Y_cv)\n",
    "prediccio = np.concatenate(prediccio)\n",
    "\n",
    "print(f'MSE: {sum((Y_cv - prediccio) ** 2) / len(prediccio)}')\n",
    "print(f'R^2: {r2_score(Y_cv, prediccio)}')\n",
    "fig = plt.figure()\n",
    "plt.xlim([0,50])\n",
    "plt.ylim([0,50])\n",
    "ax = plt.gca()\n",
    "plt.scatter(x=prediccio, y=Y_cv, s=1, alpha=.1)\n",
    "plt.plot([0, 13.5], [7.5, 7.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 7.5], [0, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([7.5, 50], [13.5, 13.5], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.plot([13.5, 13.5], [7.5, 50], color='k', linestyle='-', linewidth=2, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a143d",
   "metadata": {},
   "source": [
    "# 4. Millora dels models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399ae16",
   "metadata": {},
   "source": [
    "De tots els models vistos farem la busqueda d'hyperparametres en el del random forest i en el classificador propi.  \n",
    "Tot i que el model SVC amb kernel RBF dona també molt bons resultats triga massa amb una fracció del dataset per tal de poder fer proves per millorar el seu rendiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24667f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f05ab6f",
   "metadata": {},
   "source": [
    "# 5. Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=1e6))])\n",
    "pipe3 = Pipeline([('log', LogPipe([0, 1, 2])), ('scaler', StandardScaler()), ('model', RandomForestClassifier(n_estimators=10, max_depth=20))])\n",
    "\n",
    "atribDP = dfDP[atrib34].values\n",
    "objDP = dfDP[objP].values\n",
    "\n",
    "dades_t, objectiu_t, w_t = make_subset(atribDP, objDP)\n",
    "pipe2.fit(dades_t, objectiu_t, model__sample_weight=w_t)\n",
    "pipe3.fit(dades_t, objectiu_t, model__sample_weight=w_t)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b35c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP2 = pd.DataFrame(pipe2['model'].coef_, columns=atrib34)\n",
    "dfP2['1'] = pipe2['model'].intercept_\n",
    "dfP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "password = widgets.Text()\n",
    "warning = widgets.Label()\n",
    "strength1 = widgets.Label()\n",
    "strength2 = widgets.Label()\n",
    "strength3 = widgets.Label()\n",
    "\n",
    "def format2(x):\n",
    "    return np.array([[mitja_caracters(x), mitja_sequencia(x), aleatorietat(x), mitja_cadena(x), ponderacio_cadena(x)]])\n",
    "\n",
    "def on_value_change(change):\n",
    "    warning.value = ''\n",
    "    if len(change['new']) < 6:\n",
    "        warning.value = 'Massa curt'\n",
    "    if not isValid(change['new']):\n",
    "        password.value = change['old']\n",
    "        warning.value = 'Caracter invalid'\n",
    "    strength1.value='seguretat (metode 1): ' + str(0 if len(change['new']) < 8 else (2 if len(change['new']) > 13 else 1))\n",
    "    strength2.value='seguretat (metode 2): ' + str(pipe2.predict(format2(change['new']))[0])\n",
    "    strength3.value='seguretat (metode 3): ' + str(pipe3.predict(format2(change['new']))[0])\n",
    "\n",
    "password.observe(on_value_change, names='value')\n",
    "display(warning, password, strength1, strength2, strength3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
